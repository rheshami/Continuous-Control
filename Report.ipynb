{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enviroment\n",
    "\n",
    "In this environment, a double-jointed arm can move to target locations. A reward of +0.1 is provided for each step that the agent's hand is in the goal location. Thus, the goal of your agent is to maintain its position at the target location for as many time steps as possible.\n",
    "\n",
    "The observation space consists of 33 variables corresponding to position, rotation, velocity, and angular velocities of the arm. Each action is a vector with four numbers, corresponding to torque applicable to two joints. Every entry in the action vector should be a number between -1 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "belnds ## Learning Algorithm\n",
    "I used [Deep Deterministic Policy Gradient (DDPG)](https://arxiv.org/abs/1509.02971) algorithm. DDPG is an actor critic method where contains 4 networks: local and target Actor and local and target critic.\n",
    "- The Actor  specifies the current policy by deterministically mapping states to a specific action (approximate maximizer).  \n",
    "- The critic in ddpg is use to approximate the mazimizer over the Q values of the next state. The critcis learns to eveluate the optimal action value function by using the actors best believed action. \n",
    "- DDPG uses areply buffer.\n",
    "- DDPG soft upates the target networks. Soft updates are used to update target networks of actor and critic. Soft updates are used to slowly blends local netwroks wights woth target netwrok weights.\n",
    "- When we are training we are training the local netwroks therefore local netwroks are the most up-to-date network.\n",
    "- We use target network for predication to stablize strain.  \n",
    "- I used gradient clipping to prevent exploding gradients when training the critic network.\n",
    "- I also updated the network after 20 steps as suggested in the benchmark implementation   \n",
    "\n",
    "### Actor Network Architecture\n",
    "   - Input : 33 (state size)\n",
    "   - Output : 4 (action size)\n",
    "   - ``` \n",
    "      Actor(\n",
    "            (fc1): Linear(in_features=33, out_features=256, bias=True)\n",
    "            (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
    "            (fc3): Linear(in_features=128, out_features=4, bias=True)\n",
    "         )```\n",
    "\n",
    "\n",
    "### Critic Network Architecture\n",
    "\n",
    "    - Input : 33 (state size)\n",
    "   - Output : 4 (action size)\n",
    "   - ```\n",
    "   Critic(\n",
    "  (fcs1): Linear(in_features=33, out_features=256, bias=True)\n",
    "  (fc2): Linear(in_features=260, out_features=128, bias=True)\n",
    "  (fc3): Linear(in_features=128, out_features=1, bias=True)\n",
    ")```\n",
    "\n",
    "### Hyperparameters\n",
    "```\n",
    "BUFFER_SIZE = int(1e6)  # replay buffer size\n",
    "BATCH_SIZE = 128        # minibatch size\n",
    "GAMMA = 0.94            # discount factor\n",
    "TAU = 1e-3              # for soft update of target parameters\n",
    "LR_ACTOR = 1e-4         # learning rate of the actor \n",
    "LR_CRITIC = 1e-4        # learning rate of the critic\n",
    "WEIGHT_DECAY = 0.0      # L2 weight decay\n",
    "TRAIN_EVERY = 20        # How many iterations to wait before updating target networks\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training result:\n",
    "### Option 1\n",
    "One agent enviroment was solved in 392 Episodes with average score: 30.08\n",
    "\n",
    "![One Agent](img/oneagent.PNG)\n",
    "\n",
    "```\n",
    "Episode 100\tAverage Score: 2.09\n",
    "Episode 200\tAverage Score: 8.24\n",
    "Episode 300\tAverage Score: 18.15\n",
    "Episode 392\tAverage Score: 30.08\n",
    "```\n",
    "\n",
    "### Option 2\n",
    "Multi agent enviroument is sloved in 217 Episodes with average Score: 30.03\n",
    "\n",
    "![Multi Agent](img/multiAgent.PNG)\n",
    "\n",
    "\n",
    "```\n",
    "Episode 100\tAverage Score: 18.97\n",
    "Episode 200\tAverage Score: 29.57\n",
    "Episode 217\tAverage Score: 30.03\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Results\n",
    "\n",
    "### option 1 - One Agent\n",
    "\n",
    "```\n",
    "Episode: \t0 \tScore: \t31.29\n",
    "Episode: \t1 \tScore: \t36.03\n",
    "Episode: \t2 \tScore: \t36.61\n",
    "Episode: \t3 \tScore: \t30.97\n",
    "Episode: \t4 \tScore: \t28.38\n",
    "```\n",
    "\n",
    "### option 2 - Multi Agent\n",
    "\n",
    "```\n",
    "Episode: \t0 \tScore: \t31.60\n",
    "Episode: \t1 \tScore: \t32.11\n",
    "Episode: \t2 \tScore: \t32.80\n",
    "Episode: \t3 \tScore: \t32.55\n",
    "Episode: \t4 \tScore: \t32.23\n",
    "Episode: \t5 \tScore: \t32.77\n",
    "Episode: \t6 \tScore: \t31.62\n",
    "Episode: \t7 \tScore: \t33.29\n",
    "Episode: \t8 \tScore: \t32.15\n",
    "Episode: \t9 \tScore: \t31.52\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Ideas\n",
    "\n",
    "- Work to improve performance of the muti agent model. \n",
    "- Add bacth normalisation to the netwroks\n",
    "- Look into other models such as A3C , and PPO"
   ]
  }
 ]
}
